---
title: "Weipeng's R_markdown"
author: "Weipeng Li"
date: "2022-11-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Efficient Programming

## Memory Allocation
A good habit in R programming is to pre-allocate memory for variables before filling in values. Make sure that the size of your list or data frame does not grow after the for loop. Although R supports dynamic memory allocation when executing, it can be time-wasting in big data sets. To illustrate this, consider the different ways of creating a continuous sequence of numbers. 

The first method is to start with an empty list and gradually grow the list during the iteration.
```
method1 <- function(n) {
  vec <- c()
  for (i in seq_len(n))
    vec <- c(vec, i)
  vec
}
```

The second way is to start with a list with length $n$, and fill in the values during the iteration.
```
method2 <- function(n) {
  vec <- numeric(n)
  for (i in seq_len(n))
    vec[i] <- i
  vec
}
```

Compare the running time of the two methods. 
```
n <- 1e5
tmemory_1 <- system.time(method1(n))[3]
tmemory_2 <- system.time(method2(n))[3]

n <- 1e6
tmemory_3 <- system.time(method1(n))[3]
tmemory_4 <- system.time(method2(n))[3]
```

The following table shows the average running time on my local comupter of the two methods with different values of n. We can see from the drastic difference that as $n$ grows, the time for memory allocation can no longer be ignored. When $n=10^6$, method1 takes 36 minutes, while method2 only takes less than 1 second.

  n          Method1     Method2    
-------     ---------   ----------
  $10^5$      31.84       0.007     
  $10^6$       2155       0.063
           
Table:  Running Time (Seconds) for Different Methods of Memory Allocation

What should we do if we are not sure about the vector size before finishing the loop? In these cases it's often a good practice to allocate a large enough size for the vector, and delete the empty part after the loop. Comparing to reallocate memory for the object every time in an iteration, we only need to allocate twice. For example, we would like to find the number subarrays of $k$ number of continuous '1's in a random array. There are two ways for this.

```{r}
findones1 <- function(x,k){
  n <- length(x)
  runs <- NULL
  for(i in 1:(n-k+1)){
    if(all(x[i:(i+k-1)]==1)) runs <- c(runs,i)
  }  
  return(runs)
}
```

```{r}
findones2 <- function(x,k){
  n <- length(x)
  runs <- vector(length=n)
  count <- 0
  for(i in 1:(n-k+1)){
    if(all(x[i:(i+k-1)]==1)){
      count <- count+1
      runs[count] <- i
    }
  }
  ifelse(count > 0, runs <- runs[1:count], runs <- NULL)
  return(runs)
}
```

Test the running time of the two functions on a random sequence of length $10^6$, and set $k=4$. We can find that the first function takes a lot more time the the second function.
```{r}
set.seed(123)
n <- round(runif(n=1e6,min=0,max=1))
sprintf('Running time of findones1: %.3f',system.time(findones1(n,4))[3])
sprintf('Running time of findones2: %.3f',system.time(findones2(n,4))[3])
```

## The Apply Family

Explicit loops can be slow and verbose in R. While vectorized methods help avoiding many loops, another helpful tool is the apply family. These functions serve as nice substitutes for loops and execute efficiently. The apply functions take at least two arguments: an object and a valid R expression. The expression is applied iteratively on the objects in given orders. There are many apply functions in base R, showing in the table below. As some of them are rarely used, we will introduce some important ones in this section.

  Function        Description   
------------     -------------   
  apply()         Apply functions over array margins            
  lapply()        Apply a function over a list and return a list
  sapply()        Apply a function over a list or dataframe and return a list or a matrix
  vapply()        Apply a function over a list or dataframe and return in a given type
  mapply()        Apply a function over multiple inputs of lists or dataframes
  tapply()        Apply a function over a ragged array
  eapply()        Apply a fuction over values in an environment

Table: Functions in the Apply Family

### apply()

The apply() function applies an expression to margins of an array or matrix. It takes three arguments: X, MARGIN, and FUN. X is the array to apply the function on. MARGIN indicates the directions over which to apply the expression, with 1 for rows, 2 for columns, and c(1,2) for both rows and columns. FUN is the expression to be applied. It also takes a 'simplify' arguments, with default value of 'TRUE', indicating whether the results should be simplified. An example of apply() as an alternative of for loop is the following.

```{r}
M1 <- matrix(C<-(1:15),nrow=5)
M1_colsum<-apply(M1,2,sum)
M1_colsum
```

### lapply(), sapply()

lapply() is designed for lists. It applies a function to every entry of the input list and returns a list of the same length. sapply() works similarly as lapply(), but permits a more flexible output. By default it returns a vector or a matrix, but also supports an array output which can be controled by the 'simplify' argument.

```{r}
Names<-c("Manish","Saurabh", "Rahul","Krishna","Venkat")
Names_lower<-lapply(Names,tolower)
Names_lower
```

```{r}
Names_upper<-sapply(Names,toupper)
Names_upper
```

### tapply()
tapply() applies an operation on a subset of vector broken down by a given factor variable. It works similar to the group_by() function plus an aggregated operation in dplyr. Let's use the iris dataset as an example.

```{r}
data(iris)
summary(iris)
```
```{r}
tapply(iris$Sepal.Length,iris$Species,mean)
```

### replicate()

replicate() is a nice alternative for writing a for loop in random simulation. It executes a given program for several times without introducing a count variable. The return is a multi-dimension array of the simulating results. It takes two arguments: n and expr. n indicates the number of repetition, and expr is the operation to be executed. For example, we would like to compute the mean and standard deviation of q sequence of standard normal random variables. We decide to repeat the simulation for 8 times, and each time generate 100 random sample.

```{r}
set.seed(123)
replicate(8, {
  x <- rnorm(100, 0, 1); 
  c(mean(x), sd(x)) })
```



# Parallel Computing

Parallel computing is a widely used technique for dealing with big datasets, which uses multiple cores and threads to complete a task at the same time. While most of our computers have multiple cores, R only runs on one of the virtual cores by default.

If a large task can be divided into independent subtasks, it is easy to conduct parallel computing on different cores. The bottleneck lies in the need to communicate between cores. For example, one of the most time-consuming task in R is random simulation, where we would like to avoid using same sequences in different cores and threads.

Luckily, there are some packages for simple parallel computing in R, and we will have a quick glimpse of these functions in this section.

## General R Functions for Parallel Computing

The parallel package in R provides a simple way of parallel computing. It gives the parallel version of the apply family, namely parApply(), parLapply(), parSapply(), etc. These functions require a temporary cluster as the main object. 

To better explain the use of parallel package, let us consider a simple task. We want to compute $$S_{n,k}=\Sigma_{i=1}^{n}\frac{1}{i^k}$$ for $n=10^6$, with $k$ ranging from 2 to 21. As the computation is independent for different $k$s, it's safe to assign the task to different cores.

Let's first compute the task with a single core.
```{r}
f10 <- function(n,k){
  s <- 0.0
  for(i in seq(n)) s <- s + 1/i^k
  s
}
f11 <- function(n,nk){
  v <- sapply(2:(nk+1), function(k) f10(n,k))
  v
}
sprintf('Running time with a single core: %.3f',system.time(f11(n=1e6, nk=20))[3])
```

Now let's do the computation with multiple cores step by step. Firstly, use `detectCores()` to check the virtual cores of your computer.
```{r}
library(parallel)
nNodes <- detectCores()
nNodes
```

Next, create a temporary cluster with multiple cores which will be the main objects of our parallel computation.
```{r}
cpucl <- makeCluster(nNodes)
```

Conduct parallel computation with `parLapply()` or `parSapply()`. The computing time is now 0.818 seconds, reduced by 48% compared to the single core computation. Notice that in the parallel version, f10 has to be defined inside f12, because the function runs on different threads independently, and we have to make sure that the initialization and definition is done properly on every thread.

```{r}
f12 <- function(n,nk){
  f10 <- function(n,k){
    s <- 0.0
    for(i in seq(n)) s <- s + 1/i^k
    s
  }

  v <- parSapply(cpucl, 2:(nk+1), function(k) f10(n,k))
  v
}
sprintf('Running time with multiple cores: %.3f',system.time(f12(n=1e6, nk=20))[3])
```

Another way for the initialization is to pass the dependent objects to every cluster by `clusterExport()`. For example, instead of defining f10 inside f12, we can also do the following.
```{r}
clusterExport(cpucl, c("f10"))
f13 <- function(n,nk){
  v <- parSapply(cpucl, 2:(nk+1), function(k) f10(n,nk))
  v
}
sprintf('Running time with multiple cores: %.3f',system.time(f13(n=1e6, nk=20))[3])
```

If you need to execute some operations on every cluster node before doing the computation, `clusterEvalQ()` can be helpful. For example, you can import some certain libraries for further execution.
```{r}
clusterEvalQ(cpucl, library(dplyr))
```

After the computation, always remember to stop the cluster. This ensures efficient CPU allocation for future tasks.
```{r}
stopCluster(cpucl)
```

## Parallel Computing for Random Simulations

As we mentioned at the beginning of this section, a more challenging task for parallel computation is random simulation. For example, we want to conduct a simulation of $10^7$ times. The task can be divided into ten simulation tasks of $10^6$ times, but the random sequences should be different for the ten tasks. 

`L'Ecuyer` is a popular random number generator for parallel computation in R. It has a very long period of around $2^191$. This enables us to use a separate stream for each cluster node so that the random sequences never get into sync. `nextRNGStream()` function in the parallel package sets a specific random seed for the generator, thus assigning different streams for every node.

For example, we want to estimate the probability of a Wilson confidence interval containing the true value  $p$. Recall that the Wilson confidence interval is defined as $$\frac{\hat{p}+\frac{\lambda^2}{2n}}{1+\frac{\lambda^2}{n}} \pm \frac{\lambda}{\sqrt{n}}\frac{\sqrt{\hat{p}(1-\hat{p})+\frac{\lambda^2}{4n}}}{1+\frac{\lambda^2}{n}},$$

where $\lambda = \phi^{-1}(1-\alpha)$. We want to simulate this probability with different $alpha$, $n$ and $p$.

First, let's do the computation on a single core.

```{r}
wilson <- function(n, x, conf){
  hatp <- x/n
  lam <- qnorm((conf+1)/2)
  lam2 <- lam^2 / n
  p1 <- (hatp + lam2/2)/(1 + lam2)
  delta <- lam / sqrt(n) * sqrt(hatp*(1-hatp) + lam2/4) / (1 + lam2)
  c(p1-delta, p1+delta)
}

f20 <- function(nsim){
  set.seed(123)
  n <- 30; p0 <- 0.01; conf <- 0.95
  cover <- 0
  for(i in seq(nsim)){
    x <- rbinom(1, n, p0)
    cf <- wilson(n, x, conf)
    if(p0 >= cf[1] && p0 <= cf[2]) cover <- cover+1
  }
  cover/nsim
}
sprintf('Running time with a single core: %.3f',system.time(cvg1 <- f20(nsim=1e7))[3])
```

Now let's move on to the multi-core version of this task. With parallel computation, the task only takes 29.95s, reduced by 43% comparing to the single-core version.

```{r}
nNodes <- detectCores()
cpucl <- makeCluster(nNodes)
each.seed <- function(s){
  assign(".Random.seed", s, envir = .GlobalEnv)
}
RNGkind("L'Ecuyer-CMRG")
set.seed(123)
seed0 <- .Random.seed
seeds <- as.list(1:nNodes)

# Create different seeds for the cluster nodes
for(i in 1:nNodes){ 
  seed0 <- nextRNGStream(seed0)
  seeds[[i]] <- seed0
}
# Assign different seed for every node
junk <- clusterApply(cpucl, seeds, each.seed)
f21 <- function(isim, nsimsub){
  n <- 30; p0 <- 0.01; conf <- 0.95
  cover <- 0
  for(i in seq(nsimsub)){
    x <- rbinom(1, n, p0)
    cf <- wilson(n, x, conf)
    if(p0 >= cf[1] && p0 <= cf[2]) cover <- cover+1
  }
  cover
}
clusterExport(cpucl, c("f21", "wilson"))

f22 <- function(nsim){
  nbatch <- 40
  nsimsub <- nsim / nbatch
  cvs <- parSapply(cpucl, 1:nbatch, f21, nsimsub=nsimsub)
  sum(cvs)/(nsim*nbatch)
}
sprintf('Running time with multiple cores: %.3f',system.time(cvg2 <- f22(1e7))[3])

stopCluster(cpucl)
```


# References
[1] https://csgillespie.github.io/efficientR/programming.html
[2] https://bookdown.org/manishpatwal/bookdown-demo/
[3] https://www.math.pku.edu.cn/teachers/lidf/docs/Rbook/html/_Rbook/prog-prof.html
[4] https://rstudio.github.io/profvis/
